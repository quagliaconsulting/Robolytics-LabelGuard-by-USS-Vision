### config.yaml ###
# config.yaml

# default_model_path: "models/yolov8n.pt"
# dataset_directory: "dataset/coco128.v1i.yolov8"
# initial_iou_threshold: 0.5
# confidence_threshold: 0.2
# image_size: 224
# percentage: 100
# use_half_precision: true
# num_processes: 1
# dataset_splits:
#   - train
#   - valid
#   - test

default_model_path: "C:/Users/James/Downloads/MS-v8n-4096-0517.pt"
dataset_directory: "C:/Users/James/Downloads/MasterSplitv2.v6i.yolov8"
initial_iou_threshold: 0.5
confidence_threshold: 0.2
image_size: 4096
percentage: 10
use_half_precision: true
num_processes: 8
dataset_splits:
  - train
  - valid
  - test


### copy_util.py ###
import os
from datetime import datetime

# Get the current datetime and format it for the filename
current_datetime = datetime.now().strftime("%Y%m%d_%H%M%S")

# Define the filename for the output .txt file
output_filename = f"{current_datetime}.txt"

# Open the output file
with open(output_filename, 'w') as output_file:
    # Get the list of files in the current directory
    for file in os.listdir('.'):
        if file.endswith('.py') or file.endswith('.yaml'):
            # Write the filename as a header in the output file
            output_file.write(f"### {file} ###\n")
            # Write the contents of the file to the output file
            with open(file, 'r') as input_file:
                output_file.write(input_file.read())
            # Write a separator
            output_file.write("\n\n")

print(f"Contents of .py and .yaml files written to {output_filename}")


### image_utils.py ###
import cv2
import numpy as np
from pathlib import Path
import logging

def load_images(directory: Path):
    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
    images = [str(path) for path in directory.rglob('*') if path.suffix.lower() in image_extensions]
    return images

def draw_polygons(image_path, result, labels, prediction_boxes, class_names):
    image = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)
    if image is None:
        return None

    height, width, _ = image.shape

    # Draw all label boxes based on their status
    for label_box in labels:
        try:
            box = label_box['box']
            if isinstance(box, str) or not isinstance(box, list):
                logging.error(f"Invalid box format for label: {label_box}")
                continue

            polygon = np.array([box[:2], [box[0], box[3]], box[2:], [box[2], box[1]]])
            polygon = polygon * [width, height]
            polygon = polygon.astype(int)
            color = (0, 0, 255) if label_box.get('status') == 'FN' else (255, 0, 0)  # Red for FN, Blue for TP
            cv2.polylines(image, [polygon], isClosed=True, color=color, thickness=2)
            class_id = label_box['class']
            cv2.putText(image, class_names[class_id] if class_id < len(class_names) else f"Class {class_id}", (polygon[0][0], polygon[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        except Exception as e:
            logging.error(f"Error drawing polygon for label: {label_box}. Error: {e}")
            continue

    # Draw all prediction boxes based on their status
    for box in prediction_boxes:
        try:
            x1, y1, x2, y2 = box['box']
            x1, y1, x2, y2 = int(x1 * width), int(y1 * height), int(x2 * width), int(y2 * height)
            class_id = box['class']
            class_name = class_names[class_id] if class_id < len(class_names) else f"Class {class_id}"
            if box.get('status') == 'TP':
                color = (255, 0, 0)  # Blue for TP
            elif box.get('status') == 'FP':
                color = (0, 165, 255)  # Bright orange for FP
            else:
                color = (0, 0, 255)  # Red for FN

            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            cv2.putText(image, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        except Exception as e:
            logging.error(f"Error drawing rectangle for prediction: {box}. Error: {e}")
            continue

    return image

def draw_boxes(image_path, result, label_boxes, prediction_boxes, class_names):
    image = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)
    if image is None:
        return None

    height, width, _ = image.shape

    # Draw all label boxes based on their status
    for label_box in label_boxes:
        try:
            x1, y1, x2, y2 = label_box['box']
            x1, y1, x2, y2 = int(x1 * width), int(y1 * height), int(x2 * width), int(y2 * height)
            class_id = label_box['class']
            class_name = class_names[class_id] if class_id < len(class_names) else f"Class {class_id}"
            color = (0, 0, 255) if label_box['status'] == 'FN' else (255, 0, 0)  # Red for FN, Blue for TP
            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            cv2.putText(image, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        except ValueError:
            continue

    # Draw all prediction boxes based on their status
    for pred_box in prediction_boxes:
        try:
            x1, y1, x2, y2 = pred_box['box']
            x1, y1, x2, y2 = int(x1 * width), int(y1 * height), int(x2 * width), int(y2 * height)
            class_id = pred_box['class']
            class_name = class_names[class_id] if class_id < len(class_names) else f"Class {class_id}"
            if pred_box['status'] == 'TP':
                color = (255, 0, 0)  # Blue for TP
            elif pred_box['status'] == 'FP':
                color = (0, 165, 255)  # Bright orange for FP
            else:
                color = (0, 0, 255)  # Red for FN

            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            cv2.putText(image, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        except ValueError:
            continue

    return image


def get_bounding_box(polygon, img_width, img_height):
    x_coords = polygon[:, 0] * img_width
    y_coords = polygon[:, 1] * img_height
    x_min = int(np.min(x_coords))
    x_max = int(np.max(x_coords))
    y_min = int(np.max(x_coords))
    y_max = int(np.max(y_coords))
    return [x_min / img_width, y_min / img_height, x_max / img_width, y_max / img_height]

def calculate_iou(box1, box2):
    x_left = max(box1[0], box2[0])
    y_top = max(box1[1], box2[1])
    x_right = min(box1[2], box2[2])
    y_bottom = min(box1[3], box2[3])

    if x_right < x_left or y_bottom < y_top:
        return 0.0

    intersection_area = (x_right - x_left) * (y_bottom - y_top)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - intersection_area

    return intersection_area / union_area


### main.py ###
import streamlit as st
import yaml
import numpy as np
import tempfile
from pathlib import Path
from PIL import Image
import cv2
import streamlit_image_zoom
from image_utils import draw_polygons, draw_boxes, load_images
from model_utils import process_image, filter_mislabeled_images
from ui_components import add_logos, main_ui
import torch
import logging
from multiprocessing import Pool, Manager
from ultralytics import YOLO
from processing import model_worker
import threading
import gc
import time

# Configure logging
logging.basicConfig(filename='processing.log', level=logging.DEBUG, format='%(asctime)s - %(message)s')

def main():
    # Load configuration
    with open("config.yaml", "r") as config_file:
        config = yaml.safe_load(config_file)

    # Initialize Streamlit UI and load configuration
    add_logos()
    st.text("")
    main_ui()

    # Load class names from data.yaml
    dataset_dir = Path(config["dataset_directory"])
    with open(dataset_dir / "data.yaml", "r") as data_file:
        data_config = yaml.safe_load(data_file)
    class_names = data_config["names"]

    # Initialize the default model loaded flag
    default_model_loaded = False

    # Create a shared manager
    manager = Manager()
    progress_counters = manager.list([0] * config["num_processes"])

    # Select model
    uploaded_model = st.file_uploader("Upload your YOLOv8 model file", type=["pt"], key="model_uploader", accept_multiple_files=False)
    model_paths = []
    default_model_path = Path(config["default_model_path"])
    if uploaded_model:
        uploaded_model_content = uploaded_model.read()
        for i in range(config["num_processes"]):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pt') as tmp_model_file:
                tmp_model_file.write(uploaded_model_content)
                model_paths.append(str(tmp_model_file.name))
    else:
        for i in range(config["num_processes"]):
            model_paths.append(str(default_model_path))
        default_model_loaded = True

    # Display a message if the default model is loaded
    if default_model_loaded:
        st.info(f"Default model loaded from: {default_model_path}")
    else:
        st.success("Custom model uploaded successfully.")

    # Select dataset directory
    dataset_dir = Path(st.text_input("Enter the path to your dataset directory", config["dataset_directory"]))

    # Input for percentage of the dataset to process
    percentage = st.slider("Percentage of the dataset to process", 1, 100, config["percentage"])

    # Add a slider for confidence threshold
    conf_threshold = st.slider("Confidence Threshold", 0.0, 1.0, config["confidence_threshold"], step=0.01)

    # Add a checkbox for half precision
    use_half = st.checkbox("Use Half Precision", value=config["use_half_precision"])

    # Add an input for image size
    img_size = st.number_input("Image Size", value=config["image_size"], step=32, min_value=32)

    # Set the initial IoU threshold to 0.5 for the first run
    initial_iou_threshold = config["initial_iou_threshold"]

    # Determine device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    st.write(f"Using device: {device}")

    # Add a button to start processing images
    if st.button("Process Images"):
        if model_paths and dataset_dir:
            dataset_splits = config["dataset_splits"]

            total_images = 0
            for split in dataset_splits:
                total_images += len(load_images(dataset_dir / split / 'images'))

            num_images_to_process = int(total_images * (percentage / 100))

            all_images = []
            for split in dataset_splits:
                all_images.extend(load_images(dataset_dir / split / 'images'))

            if num_images_to_process < total_images:
                all_images = np.random.choice(all_images, num_images_to_process, replace=False).tolist()
                
            tasks = []
            images_per_model = len(all_images) // len(model_paths)

            for i, model_path in enumerate(model_paths):
                images = all_images[i * images_per_model: (i + 1) * images_per_model]
                tasks.append((model_path, images, initial_iou_threshold, conf_threshold, use_half, img_size, device, class_names, progress_counters, i))


            # Display the progress bar
            progress_bar = st.progress(0)

            # Use multiprocessing to process images in parallel
            with Pool(processes=len(model_paths)) as pool:
                results = pool.map_async(model_worker, tasks)

                while not results.ready():
                    progress_bar.progress(sum(progress_counters) / num_images_to_process)
                    time.sleep(0.1)

                pool.close()
                pool.join()

            # Aggregate results
            mislabeled_images = []
            tp, fp, fn = 0, 0, 0

            for result in results.get():
                local_mislabeled_images, local_tp, local_fp, local_fn = result
                mislabeled_images.extend(local_mislabeled_images)
                tp += local_tp
                fp += local_fp
                fn += local_fn

            # Clear large intermediate results from memory
            del all_images
            gc.collect()

            # Save results in session state
            st.session_state["mislabeled_images"] = mislabeled_images
            st.session_state["tp"] = tp
            st.session_state["fp"] = fp
            st.session_state["fn"] = fn

    # Add debug statements to the end of the image processing section
    if "mislabeled_images" in st.session_state:
        display_mode = st.radio("Display Mode", ["Polygons", "Bounding Boxes"])

        iou_threshold = st.slider("IoU Threshold", 0.0, 1.0, initial_iou_threshold, step=0.01, key="post_inference_threshold")

        mislabeled_images = st.session_state["mislabeled_images"]
        tp = st.session_state["tp"]
        fp = st.session_state["fp"]
        fn = st.session_state["fn"]

        st.write(f"Total TP: {tp}, FP: {fp}, FN: {fn}")

        filtered_mislabeled_images = filter_mislabeled_images(mislabeled_images, iou_threshold)

        logging.debug(f"Filtered Mislabeled Images: {filtered_mislabeled_images}")

        if filtered_mislabeled_images:
            st.write(f"Found {len(filtered_mislabeled_images)} potentially mislabeled images:")
            for img_path, result, labels, label_boxes, suspect_boxes, tp, fp, fn in filtered_mislabeled_images:
                st.write(f"Image: {img_path}")
                st.write(f"TP: {tp}, FP: {fp}, FN: {fn}")

                if display_mode == "Polygons":
                    image_with_annotations = draw_polygons(img_path, result, labels, suspect_boxes, class_names)
                else:
                    image_with_annotations = draw_boxes(img_path, result, label_boxes, suspect_boxes, class_names)

                if image_with_annotations is not None:
                    image_with_annotations_rgb = cv2.cvtColor(image_with_annotations, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(image_with_annotations_rgb)

                    streamlit_image_zoom.image_zoom(pil_image, mode="scroll", keep_resolution=True, size=(1024, 768), zoom_factor=5, increment=1)
                else:
                    st.write(f"Error reading image {img_path}")

                # Clear memory after displaying each image
                del image_with_annotations
                gc.collect()
        else:
            st.write("No potentially mislabeled images found.")
    else:
        st.write("Please upload a model file and enter the dataset directory to start processing images.")



if __name__ == '__main__':
    main()


### model_utils.py ###
import numpy as np
import cv2
import os
from image_utils import get_bounding_box, calculate_iou
import logging
import gc

import numpy as np
import cv2
import os
from image_utils import get_bounding_box, calculate_iou
import logging
import gc

def process_image(model, image_path, iou_threshold=0.5, conf=0.2, half=True, imgsz=None, class_names=None):
    result = None  # Initialize result to ensure it's always defined
    try:
        logging.info(f"Processing image: {image_path}")
        results = model(str(image_path), half=half, conf=conf, imgsz=imgsz)

        # Check if results are empty
        if not results or not results[0]:
            logging.warning(f"No results for image: {image_path}")
            return (image_path, None, [], [], [], 0, 0, 0)

        result = results[0]
        label_path = str(image_path).replace('images', 'labels').replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt')
        
        # Handle NULL images
        if not os.path.exists(label_path):
            logging.warning(f"Label file not found for image {image_path}. Assuming NULL labels")
            return (image_path, result, ['NULL'], [], [], 0, 1, 0)

        with open(label_path, 'r') as f:
            labels = f.readlines()

        image = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)
        if image is None:
            logging.error(f"Failed to read image {image_path}")
            return (image_path, None, [], [], [], 0, 0, 0)
        height, width, _ = image.shape

        label_bounding_boxes = []
        for label in labels:
            parts = list(map(float, label.strip().split()))
            if len(parts) != 5:
                logging.error(f"Invalid label format in file {label_path}. Label: {label.strip()}")
                continue

            class_id = int(parts[0])
            x_center, y_center, w, h = parts[1:]
            x1 = (x_center - w / 2) * width
            y1 = (y_center - h / 2) * height
            x2 = (x_center + w / 2) * width
            y2 = (y_center + h / 2) * height
            bounding_box = [x1 / width, y1 / height, x2 / width, y2 / height]
            label_bounding_boxes.append({'box': bounding_box, 'class': class_id})
            logging.debug(f"Added label bounding box: {bounding_box}, class: {class_id}")

        prediction_bounding_boxes = []
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            pred_box = [x1 / width, y1 / height, x2 / width, y2 / height]
            confidence = box.conf[0]
            class_id = int(box.cls[0])
            prediction_bounding_boxes.append({'box': pred_box, 'confidence': confidence, 'class': class_id})
        logging.debug(f"Prediction bounding boxes: {prediction_bounding_boxes}")

        tp, fp, fn = categorize_predictions(prediction_bounding_boxes, label_bounding_boxes, iou_threshold, conf, width, height)
        logging.info(f"Processed image: {image_path}, TP: {tp}, FP: {fp}, FN: {fn}")

        # Free up memory
        del image
        gc.collect()

        return (image_path, result, labels, label_bounding_boxes, prediction_bounding_boxes, tp, fp, fn)
    except Exception as e:
        logging.error(f"Error processing image {image_path}: {e}")
        return (image_path, result, [], [], [], 0, 0, 0)  # Return a tuple with a default result


def categorize_predictions(predictions, ground_truths, iou_threshold=0.5, conf_threshold=0.5, image_width=4096, image_height=3072):
    tp, fp, fn = 0, 0, 0
    matched_gts = []

    for pred in predictions:
        pred_box, pred_conf, pred_class = pred['box'], pred['confidence'], pred['class']
        logging.debug(f"Processing prediction: {pred_box}, confidence: {pred_conf}, class: {pred_class}")
        if pred_conf < conf_threshold:
            logging.debug(f"Prediction confidence {pred_conf} is below threshold {conf_threshold}")
            continue
        
        matched = False
        for gt in ground_truths:
            gt_box, gt_class = gt['box'], gt['class']
            iou = calculate_iou(pred_box, gt_box)
            logging.debug(f"Comparing with ground truth: {gt_box}, class: {gt_class}, IoU: {iou}")
            if iou >= iou_threshold and pred_class == gt_class:
                tp += 1
                matched_gts.append(gt)
                matched = True
                logging.debug(f"Match found. IoU: {iou}, class: {pred_class}. Incrementing TP")
                break

        if not matched:
            fp += 1
            logging.debug(f"No match found. Incrementing FP")

    for gt in ground_truths:
        if gt not in matched_gts:
            fn += 1
            logging.debug(f"Ground truth {gt} not matched. Incrementing FN")

    logging.info(f"Categorized predictions. TP: {tp}, FP: {fp}, FN: {fn}")
    return tp, fp, fn

def filter_mislabeled_images(mislabeled_images, threshold):
    filtered_images = []
    for img_path, result, labels, label_boxes, prediction_bounding_boxes, tp, fp, fn in mislabeled_images:
        if fp > 0 or fn > 0:
            new_suspect_boxes = []
            for box in prediction_bounding_boxes:
                x1, y1, x2, y2 = box['box']
                pred_box = [x1, y1, x2, y2]
                class_id = box['class']

                match_found = False
                for label_box in label_boxes:
                    iou = calculate_iou(pred_box, label_box['box'])
                    logging.debug(f"Comparing pred_box: {pred_box} with label_box: {label_box['box']}, IoU: {iou}")
                    if iou >= threshold and class_id == label_box['class']:
                        match_found = True
                        break
                
                if not match_found:
                    new_suspect_boxes.append({'box': pred_box, 'class': class_id})
                    logging.debug(f"Added suspect box: {pred_box}, class: {class_id}")
            
            if new_suspect_boxes:
                filtered_images.append((img_path, result, labels, label_boxes, new_suspect_boxes, tp, fp, fn))
                logging.info(f"Filtered image: {img_path}, new suspect boxes: {new_suspect_boxes}")

    return filtered_images


### processing.py ###
import sys
from ultralytics import YOLO
from image_utils import load_images
from model_utils import process_image
import logging
from multiprocessing import Pool
import gc
import yaml

# Configure logging
logging.basicConfig(filename='processing.log', level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(message)s')

def model_worker(args):
    model_path, images, iou_threshold, conf_threshold, use_half, img_size, device, class_names, progress_counters, index = args
    model = YOLO(model_path).to(device)
    local_mislabeled_images = []
    tp, fp, fn = 0, 0, 0

    for image_path in images:
        result = process_image(model, image_path, iou_threshold, conf_threshold, bool(use_half), img_size, class_names)
        if result:
            image_path, _, _, _, _, local_tp, local_fp, local_fn = result
            tp += local_tp
            fp += local_fp
            fn += local_fn
            local_mislabeled_images.append(result)
        
        # Update the progress counter
        progress_counters[index] += 1

    # Free up memory
    #del model
    #torch.cuda.empty_cache()
    gc.collect()

    return local_mislabeled_images, tp, fp, fn



if __name__ == "__main__":
    try:
        args_file = sys.argv[1]

        with open(args_file, 'r') as file:
            args = yaml.safe_load(file)

        model_paths = args["model_paths"]
        all_images = args["all_images"]
        initial_iou_threshold = args["initial_iou_threshold"]
        conf_threshold = args["conf_threshold"]
        use_half = args["use_half"]
        img_size = args["img_size"]
        num_images_to_process = args["num_images_to_process"]
        device = args["device"]
        class_names = args["class_names"]
        output_dir = args.get("output_dir", "output")

        images_per_model = len(all_images) // len(model_paths)
        tasks = []

        for i, model_path in enumerate(model_paths):
            images = all_images[i * images_per_model: (i + 1) * images_per_model]
            tasks.append((model_path, images, initial_iou_threshold, conf_threshold, use_half, img_size, device, class_names))

        # Use multiprocessing to process images in parallel
        with Pool(processes=len(model_paths)) as pool:
            logging.info("Starting multiprocessing pool")
            results = pool.map(model_worker, tasks)
            logging.info("Finished multiprocessing pool")

        mislabeled_images = []
        tp, fp, fn = 0, 0, 0

        logging.info("Starting to aggregate results")
        for result in results:
            local_mislabeled_images, local_tp, local_fp, local_fn = result
            mislabeled_images.extend(local_mislabeled_images)
            tp += local_tp
            fp += local_fp
            fn += local_fn
        logging.info("Finished aggregating results")

        output = {
            "mislabeled_images": mislabeled_images,
            "tp": tp,
            "fp": fp,
            "fn": fn
        }

        logging.info("Processing completed successfully.")
        print("done")
    except Exception as e:
        logging.error(f"Error during processing: {e}", exc_info=True)
        print(f"Error during processing: {e}")

### ui_components.py ###
import streamlit as st
import base64

def get_image_base64(path):
    with open(path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def add_logos():
    ultralytics_logo_base64 = get_image_base64('./logos/ultralytics.png')
    uss_logo_base64 = get_image_base64('./logos/uss.png')
    roboflow_logo_base64 = get_image_base64('./logos/roboflow.png')

    st.markdown(
        f"""
        <style>
        .logo-container {{
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 50px;  /* Increase spacing between images */
        }}
        .logo-container img {{
            max-height: 100%;
            max-width: 100%;
        }}
        .ultralytics-logo {{
            width: 200px;  /* Adjust width as needed */
        }}
        .uss-logo {{
            width: 200px;  /* Adjust width as needed */
        }}
        .roboflow-logo {{
            width: 200px;  /* Adjust width as needed */
        }}
        </style>
        <div class="logo-container">
            <img src="data:image/png;base64,{ultralytics_logo_base64}" class="ultralytics-logo" alt="Ultralytics">
            <img src="data:image/png;base64,{uss_logo_base64}" class="uss-logo" alt="USS Vision">
            <img src="data:image/png;base64,{roboflow_logo_base64}" class="roboflow-logo" alt="Roboflow">
        </div>
        """,
        unsafe_allow_html=True
    )

def main_ui():
    st.subheader("Robolytics LabelGuard by USS Vision", divider='rainbow')
    st.write("Identify likely mislabeled objects in your object detection dataset.")


